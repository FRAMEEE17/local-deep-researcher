{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f388a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"mcp[cli]\" requests urllib3 validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "395547a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCP Server created: SearchTheArxiv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib.parse\n",
    "import requests\n",
    "import validators\n",
    "from typing import Dict, Any, List\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"SearchTheArxiv\")\n",
    "\n",
    "print(\"MCP Server created: SearchTheArxiv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87b1d6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration for searchthearxiv MCP server\"\"\"\n",
    "    def __init__(self):\n",
    "        self.search_url = \"https://searchthearxiv.com/search\"\n",
    "        self.max_results = 10\n",
    "        self.headers = {\n",
    "            \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n",
    "            \"x-requested-with\": \"XMLHttpRequest\",\n",
    "        }\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05c2aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_query(query: str) -> bool:\n",
    "    \"\"\"Validate search query (max 200 chars as per searchthearxiv rules)\"\"\"\n",
    "    return len(query) <= 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5b608d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_paper_result(paper: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Format paper result for consistent output\"\"\"\n",
    "    return {\n",
    "        \"id\": paper.get(\"id\", \"\"),\n",
    "        \"title\": paper.get(\"title\", \"\").strip(),\n",
    "        \"authors\": paper.get(\"authors\", \"\"),\n",
    "        \"abstract\": paper.get(\"abstract\", \"\").strip(),\n",
    "        \"year\": paper.get(\"year\"),\n",
    "        \"month\": paper.get(\"month\"),\n",
    "        \"published_date\": f\"{paper.get('month', '')}-{paper.get('year', '')}\" if paper.get(\"year\") else \"\",\n",
    "        \"arxiv_url\": f\"https://arxiv.org/abs/{paper.get('id', '')}\" if paper.get(\"id\") else \"\",\n",
    "        \"pdf_url\": f\"https://arxiv.org/pdf/{paper.get('id', '')}\" if paper.get(\"id\") else \"\",\n",
    "        \"similarity_score\": paper.get(\"similarity\", 0.0)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77ddd79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_web_request(query: str, max_results: int) -> Dict[str, Any]:\n",
    "    \"\"\"Make web request to searchthearxiv API\"\"\"\n",
    "    try:\n",
    "        # Handle arXiv URL queries\n",
    "        if validators.url(query):\n",
    "            arxiv_id = query.split(\"/\")[-1]\n",
    "            search_query = arxiv_id\n",
    "        else:\n",
    "            search_query = query\n",
    "            if not validate_query(search_query):\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"error\": \"Invalid search query\"\n",
    "                }\n",
    "        encoded_query = urllib.parse.quote(search_query)\n",
    "        params = {\"query\": encoded_query}\n",
    "        \n",
    "        response = requests.get(\n",
    "            config.search_url,\n",
    "            params=params,\n",
    "            headers=config.headers,\n",
    "            timeout=30\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        result = response.json()\n",
    "        papers = result.get(\"papers\", [])\n",
    "        \n",
    "        if not papers:\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"query\": query,\n",
    "                \"total_found\": 0,\n",
    "                \"papers\": []\n",
    "            }\n",
    "        \n",
    "        # Format and limit results\n",
    "        formatted_papers = []\n",
    "        for paper in papers[:max_results]:\n",
    "            formatted_papers.append(format_paper_result(paper))\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"query\": query,\n",
    "            \"total_found\": len(formatted_papers),\n",
    "            \"papers\": formatted_papers\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": f\"Search failed: {str(e)}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44bebbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mcp.tool()\n",
    "def search_arxiv_papers(query: str, max_results: int = 10) -> str:\n",
    "    \"\"\"\n",
    "    Search for ML papers on arXiv using semantic search via searchthearxiv.com\n",
    "    \n",
    "    Args:\n",
    "        query: Search query (max 200 chars) or arXiv URL  \n",
    "        max_results: Maximum number of results (default: 10, max: 50)\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with search results\n",
    "    \"\"\"\n",
    "    if not query:\n",
    "        return json.dumps({\"success\": False, \"error\": \"Query is required\"})\n",
    "    \n",
    "    if not validate_query(query):\n",
    "        return json.dumps({\"success\": False, \"error\": \"Query too long (max 200 characters)\"})\n",
    "    \n",
    "    if max_results > 50:\n",
    "        max_results = 50\n",
    "    elif max_results < 1:\n",
    "        max_results = 1\n",
    "    \n",
    "    result = make_web_request(query, max_results)\n",
    "    return json.dumps(result, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65930b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mcp.tool()\n",
    "def get_paper_details(arxiv_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Get detailed information about a specific arXiv paper\n",
    "    \n",
    "    Args:\n",
    "        arxiv_id: arXiv paper ID (e.g., '2301.12345' or '1706.03762')\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with paper details\n",
    "    \"\"\"\n",
    "    if not arxiv_id:\n",
    "        return json.dumps({\"success\": False, \"error\": \"arXiv ID is required\"})\n",
    "    \n",
    "    # Clean the arxiv_id \n",
    "    clean_id = arxiv_id.replace(\"https://arxiv.org/abs/\", \"\").replace(\"https://arxiv.org/pdf/\", \"\")\n",
    "    \n",
    "    result = make_web_request(clean_id, 1)\n",
    "    \n",
    "    if result.get(\"success\") and result.get(\"papers\"):\n",
    "        paper = result[\"papers\"][0]\n",
    "        return json.dumps({\n",
    "            \"success\": True,\n",
    "            \"paper\": paper\n",
    "        }, indent=2)\n",
    "    else:\n",
    "        return json.dumps({\n",
    "            \"success\": False,\n",
    "            \"error\": f\"Paper {clean_id} not found\"\n",
    "            \n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a27c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mcp.tool()\n",
    "def search_by_category(category: str, query: str = \"\", max_results: int = 10) -> str:\n",
    "    \"\"\"\n",
    "    Search papers in specific ML categories\n",
    "    \n",
    "    Args:\n",
    "        category: Category (cs.CV, cs.LG, cs.CL, cs.AI, cs.NE, cs.RO)\n",
    "        query: Optional search query within category\n",
    "        max_results: Maximum number of results (default: 10)\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with search results\n",
    "    \"\"\"\n",
    "    valid_categories = [\"cs.CV\", \"cs.LG\", \"cs.CL\", \"cs.AI\", \"cs.NE\", \"cs.RO\"]\n",
    "    \n",
    "    if category not in valid_categories:\n",
    "        return json.dumps({\n",
    "            \"success\": False,\n",
    "            \"error\": f\"Invalid category. Must be one of: {', '.join(valid_categories)}\"\n",
    "        })\n",
    "    \n",
    "    # Combine category with query\n",
    "    search_query = f\"{category} {query}\".strip()\n",
    "    \n",
    "    result = make_web_request(search_query, max_results)\n",
    "    return json.dumps(result, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6e812d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mcp.tool()\n",
    "def get_trending_papers(days: int = 7, max_results: int = 10) -> str:\n",
    "    \"\"\"\n",
    "    Get trending/popular papers (simulated by searching common ML terms)\n",
    "    \n",
    "    Args:\n",
    "        days: Time period in days (ignored - API limitation)\n",
    "        max_results: Maximum number of results\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with trending papers\n",
    "    \"\"\"\n",
    "    # Simulate trending by searching popular ML terms\n",
    "    trending_queries = [\n",
    "        \"transformer neural networks\",\n",
    "        \"large language models\", \n",
    "        \"computer vision deep learning\",\n",
    "        \"reinforcement learning\",\n",
    "        \"diffusion models\"\n",
    "    ]\n",
    "    \n",
    "    import random\n",
    "    selected_query = random.choice(trending_queries)\n",
    "    \n",
    "    result = make_web_request(selected_query, max_results)\n",
    "    \n",
    "    if result.get(\"success\"):\n",
    "        result[\"note\"] = f\"Trending papers simulation using query: '{selected_query}'\"\n",
    "    \n",
    "    return json.dumps(result, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3828a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mcp.resource(\"arxiv://categories\")\n",
    "def get_categories() -> str:\n",
    "    \"\"\"List of supported arXiv categories\"\"\"\n",
    "    categories = {\n",
    "        \"cs.CV\": \"Computer Vision and Pattern Recognition\",\n",
    "        \"cs.LG\": \"Machine Learning\", \n",
    "        \"cs.CL\": \"Computation and Language\",\n",
    "        \"cs.AI\": \"Artificial Intelligence\",\n",
    "        \"cs.NE\": \"Neural and Evolutionary Computing\",\n",
    "        \"cs.RO\": \"Robotics\"\n",
    "    }\n",
    "    return json.dumps(categories, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5893a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mcp.resource(\"arxiv://stats\")\n",
    "def get_stats() -> str:\n",
    "    \"\"\"Server statistics and information\"\"\"\n",
    "    stats = {\n",
    "        \"server_name\": \"SearchTheArxiv MCP Server\",\n",
    "        \"total_papers\": \"300,000+\",\n",
    "        \"categories_supported\": 6,\n",
    "        \"embedding_model\": \"text-embedding-ada-002\",\n",
    "        \"search_engine\": \"Pinecone + OpenAI\",\n",
    "        \"data_source\": \"arXiv + Cornell University\",\n",
    "        \"update_frequency\": \"Weekly\"\n",
    "    }\n",
    "    return json.dumps(stats, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61c31fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mcp.resource(\"arxiv://help\")\n",
    "def get_help() -> str:\n",
    "    \"\"\"Help and usage information\"\"\"\n",
    "    help_info = {\n",
    "        \"available_tools\": [\n",
    "            {\n",
    "                \"name\": \"search_arxiv_papers\",\n",
    "                \"description\": \"Search for ML papers using natural language or arXiv URL\",\n",
    "                \"example\": \"search_arxiv_papers('transformer attention mechanism', 5)\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"get_paper_details\", \n",
    "                \"description\": \"Get details for a specific paper by arXiv ID\",\n",
    "                \"example\": \"get_paper_details('1706.03762')\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"search_by_category\",\n",
    "                \"description\": \"Search within specific ML categories\",\n",
    "                \"example\": \"search_by_category('cs.CV', 'object detection', 10)\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"get_trending_papers\",\n",
    "                \"description\": \"Get trending/popular papers\",\n",
    "                \"example\": \"get_trending_papers(7, 10)\"\n",
    "            }\n",
    "        ],\n",
    "        \"tips\": [\n",
    "            \"Queries are limited to 200 characters\",\n",
    "            \"Use arXiv URLs for specific paper searches\",\n",
    "            \"Supported categories: cs.CV, cs.LG, cs.CL, cs.AI, cs.NE, cs.RO\",\n",
    "            \"Results are ranked by semantic similarity\"\n",
    "        ]\n",
    "    }\n",
    "    return json.dumps(help_info, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf0abd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MCP server tools...\n",
      "\n",
      "1. Testing search_arxiv_papers:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: True\n",
      "Found 3 papers\n",
      "- AttentionViz: A Global View of Transformer Attention...\n",
      "- Attention Is All You Need...\n",
      "- Analyzing the Structure of Attention in a Transformer Langua...\n",
      "\n",
      "2. Testing get_paper_details:\n",
      "Success: True\n",
      "Title: Six-Degree-of-Freedom Motion Emulation for Data-Driven Modeling of Underwater Vehicles\n",
      "\n",
      "3. Testing search_by_category:\n",
      "Success: True\n",
      "Found 2 papers in cs.CV\n",
      "\n",
      "4. Testing resources:\n",
      "Categories: {'cs.CV': 'Computer Vision and Pattern Recognition', 'cs.LG': 'Machine Learning', 'cs.CL': 'Computation and Language', 'cs.AI': 'Artificial Intelligence', 'cs.NE': 'Neural and Evolutionary Computing', 'cs.RO': 'Robotics'}\n",
      "\n",
      "All tests completed!\n"
     ]
    }
   ],
   "source": [
    "def test_server():\n",
    "    \"\"\"Test server tools locally\"\"\"\n",
    "    print(\"Testing MCP server tools...\")\n",
    "    \n",
    "    # Test 1: Basic search\n",
    "    print(\"\\n1. Testing search_arxiv_papers:\")\n",
    "    result1 = search_arxiv_papers(\"transformer attention\", 3)\n",
    "    data1 = json.loads(result1)\n",
    "    print(f\"Success: {data1.get('success')}\")\n",
    "    if data1.get('success'):\n",
    "        print(f\"Found {data1.get('total_found')} papers\")\n",
    "        for paper in data1.get('papers', []):\n",
    "            print(f\"- {paper.get('title', '')[:60]}...\")\n",
    "    \n",
    "    # Test 2: Paper details\n",
    "    print(f\"\\n2. Testing get_paper_details:\")\n",
    "    result2 = get_paper_details(\"1706.03762\")  # Attention is All You Need\n",
    "    data2 = json.loads(result2)\n",
    "    print(f\"Success: {data2.get('success')}\")\n",
    "    if data2.get('success'):\n",
    "        paper = data2.get('paper', {})\n",
    "        print(f\"Title: {paper.get('title', '')}\")\n",
    "    \n",
    "    # Test 3: Category search\n",
    "    print(f\"\\n3. Testing search_by_category:\")\n",
    "    result3 = search_by_category(\"cs.CV\", \"object detection\", 2)\n",
    "    data3 = json.loads(result3)\n",
    "    print(f\"Success: {data3.get('success')}\")\n",
    "    if data3.get('success'):\n",
    "        print(f\"Found {data3.get('total_found')} papers in cs.CV\")\n",
    "    \n",
    "    # Test 4: Resources\n",
    "    print(f\"\\n4. Testing resources:\")\n",
    "    categories = get_categories()\n",
    "    print(f\"Categories: {json.loads(categories)}\")\n",
    "    \n",
    "    print(\"\\nAll tests completed!\")\n",
    "\n",
    "# Run tests\n",
    "test_server()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2504270c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SEARCHTHEARXIV MCP SERVER\n",
      "============================================================\n",
      "üìö Semantic search over 300,000+ ML papers\n",
      "üîç Categories: cs.CV, cs.LG, cs.CL, cs.AI, cs.NE, cs.RO\n",
      "ü§ñ Powered by OpenAI + Pinecone\n",
      "üìñ Data from arXiv + Cornell University\n",
      "\n",
      "üõ†Ô∏è  Available Tools:\n",
      "- search_arxiv_papers(query, max_results)\n",
      "- get_paper_details(arxiv_id)\n",
      "- search_by_category(category, query, max_results)\n",
      "- get_trending_papers(days, max_results)\n",
      "\n",
      "üìã Available Resources:\n",
      "- arxiv://categories - List supported categories\n",
      "- arxiv://stats - Server statistics\n",
      "- arxiv://help - Help and usage info\n",
      "result = await session.call_tool(\"search_arxiv_papers\", {\"query\": \"transformer\", \"max_results\": 5})\n",
      "Starting SearchTheArxiv MCP Server...\n",
      "\n",
      "Server configuration ready!\n",
      "Tools registered: 4\n",
      "Resources registered: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SEARCHTHEARXIV MCP SERVER\")\n",
    "print(\"=\"*60)\n",
    "print(\"üìö Semantic search over 300,000+ ML papers\")\n",
    "print(\"üîç Categories: cs.CV, cs.LG, cs.CL, cs.AI, cs.NE, cs.RO\") \n",
    "print(\"ü§ñ Powered by OpenAI + Pinecone\")\n",
    "print(\"üìñ Data from arXiv + Cornell University\")\n",
    "\n",
    "print(f\"\\nüõ†Ô∏è  Available Tools:\")\n",
    "print(\"- search_arxiv_papers(query, max_results)\")\n",
    "print(\"- get_paper_details(arxiv_id)\")\n",
    "print(\"- search_by_category(category, query, max_results)\")\n",
    "print(\"- get_trending_papers(days, max_results)\")\n",
    "\n",
    "print(f\"\\nüìã Available Resources:\")\n",
    "print(\"- arxiv://categories - List supported categories\")\n",
    "print(\"- arxiv://stats - Server statistics\")  \n",
    "print(\"- arxiv://help - Help and usage info\")\n",
    "\n",
    "print('result = await session.call_tool(\"search_arxiv_papers\", {\"query\": \"transformer\", \"max_results\": 5})')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This allows the server to be run directly\n",
    "    print(\"Starting SearchTheArxiv MCP Server...\")\n",
    "\n",
    "\n",
    "    print(\"\\nServer configuration ready!\")\n",
    "    print(f\"Tools registered: {len(mcp._tool_manager._tools)}\")\n",
    "    print(f\"Resources registered: {len(mcp._resource_manager._resources)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd42d13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
